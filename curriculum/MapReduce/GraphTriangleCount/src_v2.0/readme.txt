2.0版本，在1.0的版本上进行了数据储存和表示方面的优化：
1、将离散化稀疏的节点转换成顺序化的，这样就可以用IntWritabel表示节点(前提是节点数未超过INT_MAX)而不用Text来表示节点编号，这样就可以避免字符串排序，减少map和reduce阶段的排序开销
2、将原始Text输入文件转换成Sequence，因为hadoop传输在网络上的数据是序列化的，这样可以避免数据的序列化转换开销。但是由于数据是串行转换的，影响整体性能，但是可以在第一次运行过后存起来，以后运行直接加载sequence的数据文件即可。这一步是和第一步顺序化节点一起完成的，转换后的sequence文件存储的是顺序化的节点表示的边。
3、在a->b and b->a then a-b的条件下，在获取小度指向大度的边集任务中，mapper需要将一条边的点对合并为key以在reducer中判断是否是双向边，看似只能用Text来存储了，实则这里有一个trick，在对节点顺序化之后的节点数通常不会超过INT_MAX，因此可以使用考虑将两个int型表示的节点转换成long,key存储在高32位，value存储在低32位，通过简单的位操作即可实现，这样mapper输出的key就是long而非Text，从而避免了字符串的比较排序，由于mapreduce涉及大量排序过程，因此在涉及程序的时候尽量用一些trick避免用Text表示key.
